import os
import numpy as np

from classifier_lewidi import *
from params_lewidi import *
from DataManager import *

def main():
    
    filepath1 = './data/ConvAbuse_dataset/'
    filepath2 = './data/ArMIS_dataset/'
    filepath3 = './data/HS-Brexit_dataset/'
    filepath4 = './data/MD-Agreement_dataset/'
    data = DataManager(filepath1, filepath2, filepath3, filepath4)
    def_params = params_lewidi()
    train_dat = ["conv_train", "ar_train", "br_train", "md_train"]
    dev_dat = ["conv_dev", "ar_dev", "br_dev", "md_dev"]
    test_dat = ["conv_test", "ar_test", "br_test", "md_test"]

    
    names = ["CONV-AI","ARABIC","BREXIT","MDDATASET"]
    for i in range(len(train_dat)):
        key_train = train_dat[i]
        key_dev = dev_dat[i]
        key_test = test_dat[i] 
        if key_train == "ar_train":
            def_params.ar_dat = 1
        # elif key_train == "md_train":
        #     def_params.task = "single"

        print("-------------------------------------------------------------------------------------------------------") 
        print("-----------------------------WE ARE IN THE", names[i], "DATASET---------------------------------------------")
        if key_train != "md_train":
            annotators = list(sorted(set(itertools.chain.from_iterable(data.dataset_groups[key_train]['annotators'].str.findall("\w+")))))
        else:
            annotators = [x for x in list(data.dataset_groups[key_train].columns.values) if x.startswith('Ann')]
            
        model = AbuseClassifier(data.dataset_groups[key_train], data.dataset_groups[key_dev], data.dataset_groups[key_test] , annotators=annotators, params=def_params)
        results = model.CV()

        comp_metrics = results
        comp_metrics["soft_label_0"] = np.nan
        comp_metrics["soft_label_1"] = np.nan
        comp_metrics["hard_label"] = np.nan

        for ind, row in results.iterrows():
            
            count_zero, count_one, totalcount = 0,0,0
            for element in row:
                if np.isnan(element):
                    continue
                elif element == 0:
                    count_zero+= 1 
                    totalcount +=1
                else:
                    count_one +=1
                    totalcount +=1   
                        
            soft_zero, soft_one = count_zero / float(totalcount) , count_one / float(totalcount)

            comp_metrics["soft_label_0"][ind] = soft_zero
            comp_metrics["soft_label_1"][ind] = soft_one

            if soft_zero > soft_one:
                comp_metrics["hard_label"][ind] = 0
            elif soft_zero < soft_one:
                comp_metrics["hard_label"][ind] = 1
            else:
                comp_metrics["hard_label"][ind] = random.randrange(0, 2)
                
        comp_metrics = comp_metrics.loc[:,["hard_label","soft_label_0", "soft_label_1"]]
        comp_metrics["hard_label"]= pd.to_numeric(comp_metrics["hard_label"], downcast="integer")
        results_filepath = "../data/results/"
        os.makedirs(results_filepath, exist_ok=True) 
        if key_test == "conv_test":
            comp_metrics.to_csv(results_filepath + "ConvAbuse_results.tsv", sep="\t", header = False,index=False)
        elif key_test == "ar_test":
            comp_metrics.to_csv(results_filepath + "ArMIS_results.tsv", sep="\t", header = False,index=False)
        elif key_test == "br_test":
            comp_metrics.to_csv(results_filepath + "HS-Brexit_results.tsv", sep="\t", header = False,index=False)
        elif key_test == "md_test":
            comp_metrics.to_csv(results_filepath + "MD-Agreement_results.tsv", sep="\t", header = False,index=False)
        print("---------------------------------------", names[i], "DATASET END--------------------------------------------")
        print("-------------------------------------------------------------------------------------------------------")
            
if __name__ == "__main__":
    main()    